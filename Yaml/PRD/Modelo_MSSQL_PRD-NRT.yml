# Desenvolvido pelo time da Fábrica de Ingestão ATRA Consultoria
# [MSSQL] Processo Near Real Time (NRT) para leitura dos dados de tabelas transacionais

main:
  steps:

    # Atribui os valores iniciais para as variáveis
    - init:
        assign:
          - project_id: ${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
          - location: "southamerica-east1"
          - temp_location: "gs://workflow-core-data-us-east1/workflow-jobs/frente_migracao_dados_historicos_oracle"
          - template_path: ${"gs://dataflow-templates-" + location + "/latest/flex/SQLServer_to_BigQuery"}
          - service_account: "csf-core-data-workflow@csf-core-data-prd.iam.gserviceaccount.com"
          - KMSEncryptString: "projects/csf-core-data-prd/locations/global/keyRings/MSSQL_Ring/cryptoKeys/MSSQL_Key"
          - databaseName: "Credito" # Parâmetro do processo
          - connectionPropertiesString: ${"database=" + databaseName + ";encrypt=true;trustServerCertificate=true"}
          - username_token_id: "MSSQL_PRD_Username" # Parâmetro do processo
          - password_token_id: "MSSQL_AGSQLPRP_PRD_Psw" # Parâmetro do processo
          - connection_token_id: "MSSQL_PRD_AGSQLPRP_ConnectionString" # Parâmetro do processo
          - job_name: "p0028it-pr-credito-tb-titular-info-profissional-exec" # Parâmetro do processo
          - outputTableName: "pr_credito_tb_titular_info_profissional" # Parâmetro do processo
          - preSQLDatetime: ${"SELECT IFNULL(FORMAT_TIMESTAMP('%Y-%m-%d %H:%M:%E3S', MAX(dh_criac)), '1900-01-01 00:00:00.000'), IFNULL(FORMAT_TIMESTAMP('%Y-%m-%d %H:%M:%E3S', MAX(dh_alt)), '1900-01-01 00:00:00.000') FROM " + project_id + ".raw." + outputTableName} # Parâmetro do processo
          - preSQLTruncate: ${"TRUNCATE TABLE " + project_id + ".staging." + outputTableName}
          - querySelect: "SELECT id_titinfoprofis, cd_atvdeprofistit, cd_profistit, nm_emprstit, nu_cnpjemprstit, id_titende, CONVERT(VARCHAR, dt_admittit, 121) AS dt_admittit, vl_rendainfotit, vl_rendacomprovtit, cd_tipocomprovrenda, nu_beneftit, nm_usucriac, nm_usualt, CONVERT(VARCHAR, dh_criac, 121) AS dh_criac, CONVERT(VARCHAR, dh_alt, 121) AS dh_alt, cd_drttit, vl_rendautiltit, fg_rendacomprovtit" # Parâmetro do processo
          - queryFrom: " FROM TB_TITULAR_INFO_PROFISSIONAL WITH (NOLOCK)" # Parâmetro do processo
          - queryWhere: " WHERE (dh_criac > CONVERT(DATETIME, '" # Parâmetro do processo
          - queryControle1: "', 121) AND dh_criac < GETDATE()) OR (dh_alt > CONVERT(DATETIME, '" # Parâmetro do processo
          - queryControle2: "', 121) AND dh_alt < GETDATE())" # Parâmetro do processo
          - outputTableString: ${project_id + ":staging." + outputTableName}
          - failureStatuses: ["JOB_STATE_FAILED", "JOB_STATE_CANCELLED", "JOB_STATE_UPDATED", "JOB_STATE_DRAINED"]
          - currentStatus: ""

    # Obtém o usuário da conexão
    - get_username_secret:
        call: googleapis.secretmanager.v1.projects.secrets.versions.accessString
        args:
          secret_id: ${username_token_id}
          project_id: ${project_id}
        result: user_connection

    # Obtém a senha da conexão
    - get_password_secret:
        call: googleapis.secretmanager.v1.projects.secrets.versions.accessString
        args:
          secret_id: ${password_token_id}
          project_id: ${project_id}
        result: pass_connection

    # Obtém o token da conexão
    - get_connection_secret:
        call: googleapis.secretmanager.v1.projects.secrets.versions.accessString
        args:
          secret_id: ${connection_token_id}
          project_id: ${project_id}
        result: connect_connection

    # Aguarda o tempo determinado para evitar conflito com o job anterior
    - sleep:
        call: sys.sleep
        args:
          seconds: 60

    # Extrai a última data carregada
    - select_datetime:
        call: googleapis.bigquery.v2.jobs.query
        args:
          projectId: ${project_id}
          body:
            useLegacySql: false
            useQueryCache: false
            timeoutMs: 180000
            query: ${preSQLDatetime}
        result: datetime

    # Inclui os valores de data extraidos na query
    - intermediate:
        assign:
          - datetime1: ${datetime.rows[0].f[0].v}
          - datetime2: ${datetime.rows[0].f[1].v}
          - queryString: ${querySelect + queryFrom + queryWhere + datetime1 + queryControle1 + datetime2 + queryControle2}

    # Deleta todos os registros da tabela de destino
    - truncate_output_table:
        call: googleapis.bigquery.v2.jobs.query
        args:
          projectId: ${project_id}
          body:
            useLegacySql: false
            useQueryCache: false
            timeoutMs: 180000
            query: ${preSQLTruncate}

    # Gera o job Dataflow
    - create_job:
        call: googleapis.dataflow.v1b3.projects.locations.flexTemplates.launch
        args:
          projectId: ${project_id}
          location: ${location}
          body:
            launchParameter:
              jobName: ${job_name}
              containerSpecGcsPath: ${template_path}
              environment:
                tempLocation: ${temp_location}
                serviceAccountEmail: ${service_account}
                subnetwork: "https://www.googleapis.com/compute/v1/projects/csf-net-sharedvpc-prd-1/regions/southamerica-east1/subnetworks/csf-subnet-prd-core-data-resources-sa-east1"
                network: "csf-net-sharedvpc-prd-1"
              parameters:
                useStorageWriteApi: "true"
                useColumnAlias: "true"
                connectionURL: ${connect_connection}
                query: ${queryString}
                outputTable: ${outputTableString}
                bigQueryLoadingTemporaryDirectory: ${temp_location}
                username: ${user_connection}
                password: ${pass_connection}
                KMSEncryptionKey: ${KMSEncryptString}
                connectionProperties: ${connectionPropertiesString}
                numWorkers: "1"
                maxNumWorkers: "1"
                workerMachineType: "n1-standard-16"
                autoscalingAlgorithm: "NONE"
        result: launchResponse

    # Verifica o status do job Dataflow
    - check_job_status:
        switch:
          - condition: ${currentStatus in failureStatuses}
            next: assign_failure
          - condition: ${currentStatus != "JOB_STATE_DONE"}
            next: iterate
        next: assign_success

    # Obtém o status do job Dataflow
    - iterate:
        steps:
          - sleep_iterate:
              call: sys.sleep
              args:
                seconds: 60
          - get_job_status:
              call: googleapis.dataflow.v1b3.projects.locations.jobs.get
              args:
                jobId: ${launchResponse.job.id}
                location: ${launchResponse.job.location}
                projectId: ${launchResponse.job.projectId}
                view: "JOB_VIEW_SUMMARY"
              result: getJobResponse
          - getStatus:
              assign:
                - currentStatus: ${getJobResponse.currentState}
        next: check_job_status

    # Atribui o valor de "Sucesso" para a saída
    - assign_success:
        assign:
          - outputStatus: "Sucesso"
        next: the_end

    # Atribui o valor de "Falha" para a saída
    - assign_failure:
        assign:
          - outputStatus: "Falha"

    # Retorna o valor da saída
    - the_end:
        return: ${outputStatus}