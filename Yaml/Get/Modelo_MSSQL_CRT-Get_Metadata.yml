# Desenvolvido pelo time da Fábrica de Ingestão ATRA Consultoria.
# [MSSQL] Processo de captura dos metadados da tabela transacional

main:
  steps:

    # Atribui os valores iniciais para as variáveis
    - init:
        assign:
          - project_id: ${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
          - location: "southamerica-east1"
          - temp_location: ${"gs://dataflow-staging-" + location + "-1080416849631/tmp"}
          - template_path: ${"gs://dataflow-templates-" + location + "/latest/flex/SQLServer_to_BigQuery"}
          - service_account: "service-1080416849631-dataflow@csf-core-data-crt.iam.gserviceaccount.com"
          - KMSEncryptString: "projects/csf-core-data-crt/locations/global/keyRings/MSSQL_Ring/cryptoKeys/MSSQL_Key"
          - databaseName: "MS_ContaDigital" # Parâmetro do processo
          - connectionPropertiesString: ${"database=" + databaseName + ";encrypt=true;trustServerCertificate=true"}
          - username_token_id: "MSSQL_CRT_Username" # Parâmetro do processo
          - password_token_id: "MSSQL_SQL005_CRT_Psw" # Parâmetro do processo
          - connection_token_id: "MSSQL_CRT_SQL005_ConnectionString" # Parâmetro do processo
          - job_name: "ewally-contadigital-tb-status-cartao-pre-pago-get-metadata-crt" # Parâmetro do processo
          - sourceTableName: "TB_STATUS_CARTAO_PRE_PAGO" # Parâmetro do processo
          - querySelect: "SELECT CONCAT(META.TABLE_CATALOG, '.', META.TABLE_SCHEMA, '.', META.TABLE_NAME) AS TABELA, TABLE_DESC.value AS DESCRICAO_TABELA, META.COLUMN_NAME AS COLUNA, COLUMN_DESC.value AS DESCRICAO_COLUNA, META.DATA_TYPE AS TIPO, CASE WHEN META.DATETIME_PRECISION IS NOT NULL THEN CONVERT(VARCHAR, META.DATETIME_PRECISION) WHEN META.NUMERIC_PRECISION IS NOT NULL THEN CONCAT(META.NUMERIC_PRECISION, ',', META.NUMERIC_SCALE) ELSE CONVERT(VARCHAR, META.CHARACTER_MAXIMUM_LENGTH) END AS TAMANHO, META.COLUMN_DEFAULT AS VALOR_DEFAULT, META.IS_NULLABLE AS PERMITE_NULL, META.ORDINAL_POSITION AS POSICAO_COLUNA, CAST(IND.is_unique AS VARCHAR) AS DADO_UNICO, CONS.CONSTRAINT_TYPE AS TIPO_CONSTRAINT, CONS.CONSTRAINT_NAME AS NOME_CONSTRAINT, IND.name AS NOME_INDICE, IND_COLUMNS.key_ordinal AS POSICAO_INDICE, CONVERT(VARCHAR, GETDATE(), 121) AS DATAHORA_ATUAL"
          - queryFrom: " FROM INFORMATION_SCHEMA.COLUMNS AS META LEFT JOIN sys.extended_properties AS TABLE_DESC ON TABLE_DESC.major_id = OBJECT_ID(META.TABLE_SCHEMA + '.' + META.TABLE_NAME) AND TABLE_DESC.minor_id = 0 AND TABLE_DESC.name = 'MS_Description' LEFT JOIN sys.extended_properties AS COLUMN_DESC ON COLUMN_DESC.major_id = OBJECT_ID(META.TABLE_SCHEMA + '.' + META.TABLE_NAME) AND COLUMN_DESC.minor_id = COLUMNPROPERTY(OBJECT_ID(META.TABLE_SCHEMA + '.' + META.TABLE_NAME), META.COLUMN_NAME, 'ColumnId') AND COLUMN_DESC.name = 'MS_Description' LEFT JOIN (SELECT KCU.TABLE_SCHEMA, KCU.TABLE_NAME, KCU.COLUMN_NAME, TC.CONSTRAINT_TYPE, TC.CONSTRAINT_NAME FROM INFORMATION_SCHEMA.KEY_COLUMN_USAGE KCU JOIN INFORMATION_SCHEMA.TABLE_CONSTRAINTS TC ON KCU.CONSTRAINT_SCHEMA = TC.CONSTRAINT_SCHEMA AND KCU.CONSTRAINT_NAME = TC.CONSTRAINT_NAME AND KCU.TABLE_NAME = TC.TABLE_NAME) AS CONS ON META.TABLE_SCHEMA = CONS.TABLE_SCHEMA AND META.TABLE_NAME = CONS.TABLE_NAME AND META.COLUMN_NAME = CONS.COLUMN_NAME LEFT JOIN sys.index_columns AS IND_COLUMNS ON IND_COLUMNS.object_id = OBJECT_ID(META.TABLE_SCHEMA + '.' + META.TABLE_NAME) AND IND_COLUMNS.column_id = COLUMNPROPERTY(OBJECT_ID(META.TABLE_SCHEMA + '.' + META.TABLE_NAME), META.COLUMN_NAME, 'ColumnId') LEFT JOIN sys.indexes AS IND ON IND.object_id = IND_COLUMNS.object_id AND IND.index_id = IND_COLUMNS.index_id"
          - queryWhere: ${" WHERE META.TABLE_NAME = '" + sourceTableName + "'"}
          - queryString: ${querySelect + queryFrom + queryWhere}
          - outputTableString: ${project_id + ":staging.fabrica_de_ingestao_get_metadata"}
          - failureStatuses: ["JOB_STATE_FAILED", "JOB_STATE_CANCELLED", "JOB_STATE_UPDATED", "JOB_STATE_DRAINED"]
          - currentStatus: ""
          - posSQLOutput: ${"SELECT * FROM " + project_id + ".staging.fabrica_de_ingestao_get_metadata WHERE TABELA LIKE '%" + databaseName + "%" + sourceTableName + "' QUALIFY RANK() OVER(PARTITION BY TABELA ORDER BY DATAHORA_ATUAL DESC) = 1 ORDER BY POSICAO_COLUNA"}
          - outputFilePath: ${"fabrica_de_ingestao_get_metadata/" + databaseName + "_" + sourceTableName + "-" + text.replace_all(text.substring(time.format(sys.now()), 0, 19), ":", "-") + ".json"}

    # Obtém o usuário da conexão
    - get_username_secret:
        call: googleapis.secretmanager.v1.projects.secrets.versions.accessString
        args:
          secret_id: ${username_token_id}
          project_id: ${project_id}
        result: user_connection

    # Obtém a senha da conexão
    - get_password_secret:
        call: googleapis.secretmanager.v1.projects.secrets.versions.accessString
        args:
          secret_id: ${password_token_id}
          project_id: ${project_id}
        result: pass_connection

    # Obtém o token da conexão
    - get_connection_secret:
        call: googleapis.secretmanager.v1.projects.secrets.versions.accessString
        args:
          secret_id: ${connection_token_id}
          project_id: ${project_id}
        result: connect_connection

    # Gera o job Dataflow
    - create_job:
        call: googleapis.dataflow.v1b3.projects.locations.flexTemplates.launch
        args:
          projectId: ${project_id}
          location: ${location}
          body:
            launchParameter:
              jobName: ${job_name + "-" + text.substring(time.format(sys.now()), 0, 10)}
              containerSpecGcsPath: ${template_path}
              environment:
                tempLocation: ${temp_location}
                serviceAccountEmail: ${service_account}
                subnetwork: "https://www.googleapis.com/compute/v1/projects/csf-net-sharedvpc-crt-1/regions/southamerica-east1/subnetworks/csf-subnet-crt-core-data-resources-sa-east1"
                network: "csf-net-sharedvpc-crt-1"
              parameters:
                useStorageWriteApi: "true"
                useColumnAlias: "true"
                connectionURL: ${connect_connection}
                query: ${queryString}
                outputTable: ${outputTableString}
                bigQueryLoadingTemporaryDirectory: "gs://csf-core-data-crt/teste_dataflow_tmp/"
                username: ${user_connection}
                password: ${pass_connection}
                KMSEncryptionKey: ${KMSEncryptString}
                connectionProperties: ${connectionPropertiesString}
                numWorkers: "1"
                maxNumWorkers: "1"
                workerMachineType: "n1-standard-8"
        result: launchResponse

    # Verifica o status do job Dataflow
    - check_job_status:
        switch:
          - condition: ${currentStatus in failureStatuses}
            next: assign_failure
          - condition: ${currentStatus != "JOB_STATE_DONE"}
            next: iterate
        next: assign_success

    # Obtém o status do job Dataflow
    - iterate:
        steps:
          - sleep_iterate:
              call: sys.sleep
              args:
                seconds: 60
          - get_job_status:
              call: googleapis.dataflow.v1b3.projects.locations.jobs.get
              args:
                jobId: ${launchResponse.job.id}
                location: ${launchResponse.job.location}
                projectId: ${launchResponse.job.projectId}
                view: "JOB_VIEW_SUMMARY"
              result: getJobResponse
          - getStatus:
              assign:
                - currentStatus: ${getJobResponse.currentState}
        next: check_job_status

    # Atribui o valor de "Sucesso" para a saída
    - assign_success:
        assign:
          - outputStatus: "Sucesso"
        next: select_output

    # Atribui o valor de "Falha" para a saída
    - assign_failure:
        assign:
          - outputStatus: "Falha"
        next: the_end

    # Extrai os metadados obtidos
    - select_output:
        call: googleapis.bigquery.v2.jobs.query
        args:
          projectId: ${project_id}
          body:
            useLegacySql: false
            useQueryCache: false
            timeoutMs: 30000
            query: ${posSQLOutput}
        result: output

    # Grava um arquivo Json com os metadados obtidos
    - upload_file_to_bucket:
        call: googleapis.storage.v1.objects.insert
        args:
          bucket: ${project_id}
          name: ${outputFilePath}
          body:
            content: ${output}
          uploadType: "media"

    # Retorna o valor da saída
    - the_end:
        return: ${outputStatus}